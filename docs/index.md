# DCAN Labs NDA BIDS Uploading Tools

This README covers four data preparation steps and has an appendix.

**Table of Contents**

1. File naming conventions
1. Preparing Lookup CSV
1. Preparing Content YAML and JSON files
1. Preparing to use scripts.
1. Using Prepare and Upload Scripts
1. Appendix

## 1. File naming conventions

YAML and JSON files are always paired. If a YAML file is created then a JSON must be made with the same file name.

### File naming Format

**`A_X.Y.Z`**

When creating JSON and YAML files there is an expected naming convention.  The naming convention has four "sections", `A_X.Y.Z` with `.yaml` or `.json` on the end depending on the type of file that is being created.

#### For all sections

There are two restrictions on naming conventions.

1. Periods `"."` should only be used in the separations between sections `X`, `Y` and `Z`.
2. Underscores `"_"` should only be used in EITHER the separation between sections `A` and `X` OR in the naming of section `Z`.

#### Section `A`

Must be:

* `fmriresults01`
* `image03`

The `fmriresults01` data structure should be used for any processed MRI or fMRI data with the NDA.  The `imagingcollection01` data structure is being phased out by the NDA for non-HCP and non-ABCD studies.  `imagingcollection01` is therefore deprecated within these tools. For non-HCP and non-ABCD studies use `fmriresults01` for all none source data. For all source use the `image03` data structure.

#### Section `X`

Can be:

* `inputs`
* `derivatives`
* `sourcedata`

For section `X` there are currently these options.  `inputs` are the as-acquired data in BIDS format.  `derivatives` are the results of processing inputs.  `sourcedata` are things like task timing files and other raw/as-acquired data.

#### Section `Y`

Usually BIDS standard data naming, but is allowed to deviate:

* `anat`
* `dwi`
* `fmap`
* `func`
* `executivesummary`
* (similar entries)

There are BIDS naming conventions for some of these entities.  Other naming conventions can be created as necessary.

#### Section `Z`

An open-ended and user-defined data subset type.  It is recommended to use something concise enough to convey the contents of what you've prepared.

## 2. Preparing Lookup CSV

The Lookup CSV has six columns and `N+1` rows where `N` is the number of subject-session pairings. The first row MUST be a header row with this exact content.

```ascii
bids_subject_session,subjectkey,src_subject_id,interview_date,interview_age,sex
```

Each column should hold the information of their row's subject-session pairing.

### `bids_subject_session`

This is the exact label used above in Section `S`.

**`sub-<subjectlabel>[_ses-<sessionlabel>]`**

Where:

* `<subjectlabel>` needs to be replaced by your actual BIDS-standard subject label
* `<sessionlabel>` needs to be replaced by your actual BIDS-standard session label
* The square brackets around `[_ses-<sessionlabel>]` implies this block is optional, but it should be used if you have multiple sessions for single subjects within a dataset.

Remember: BIDS labels (`<subjectlabel>` and `<sessionlabel>`) are **ONLY** alphanumeric.  Spaces, underscores, hyphens, and any other seperators are **NOT ALLOWED**.

### `subjectkey`

This is generated by the NDA GUID tool and always starts with `NDAR`.  It is a globaly unique identifier for the subject.

### `src_subject_id`

This is the subject ID that was used by the lab or project.

### `interview_date`

The date on which the interview/genetic test/sampling/imaging/biospecimen was completed.  It **MUST** be in the format `MM/DD/YYYY`.

### `interview_age`

The age in months of the subject at the time of the interview_date. This value should always be an integer.

### `sex`

The sex of the subject. There are 4 currently accepted values

* `M` = Male
* `F` = Female
* `O` = Other
* `NR` = Not Reported

## 3. Preparing Content YAML and File Mapper JSON files

The content YAML's and file mapper JSON's are used in the records.py and prepare.py scrits respectively. The YAML's are used to create the appropriate data structrures for upload. The JSON's are used to map the data into a BIDS format.

### YAML files

The content YAML files are used together with the lookup CSV to construct the `fmriresults01`, `image03`, or `imagingcollection01` CSV files.  There are three categories that these fields fall into: **Required**, **Conditional**, and **Recommended**.  Required fields **MUST** be filled with NDA-valid information.  Conditional fields **MUST** be filled if their condition is met.  Recommended fields do not need to be filled, but should be filled if information is available.

There are three YAML files provided.

* `fmriresults01_required.yaml`
* `fmriresults01_complete.yaml`
* `fmriresults01_full_NDA.yaml`

The pair of **complete** and **required** yaml files are used to provide relavent information to **prepare.py**.  These files **MUST** stay in the `content` directory under `nda-bids-upload-prepare`.

### [fmriresults01](https://nda.nih.gov/data_structure.html?short_name=fmriresults01)

There are values found on the NDA site that are not listed below. That is because `records.py` uses the provided files to generate the values stored in the fields `manifest` and `image_description`.

#### Required

* `file_source`
* `pipeline`
* `pipeline_script`
* `pipeline_tools`
* `pipeline_type`
* `pipeline_version`
* `qc_fail_quest_reason`
* `qc_outcome`
* `scan_type`

#### Conditional

* `derived_files`

The field `derived_files` is conditional on whether or not the field `manifest` is filled.  Since `records.py` generates and fills the `manifest` field, `derived_files` is unnecessary.

#### Recommended

* `origin_dataset_id`
* `experiment_id`
* `inputs`
* `img03_id`
* `job_name`
* `proc_types`
* `metric_files`
* `img03_id2`
* `file_source2`
* `session_det`
* `image_history`

### JSON files

The file mapper JSON files is used together with the lookup CSV and the target directory passed to the prepare.py script and are used together to create BIDS formated directories.

#### Example 1: file mapper JSON

```ascii
{
    "CHANGES": "CHANGES",
    "README": "README",
    "dataset_description.json": "dataset_description.json",
    "HCP/derivatives/abcd-hcp-pipeline/sub-{SUBJECTLABEL}/[ses-{SESSIONLABEL}/]anat/sub-{SUBJECTLABEL}[_ses-{SESSIONLABEL}]_space-ACPC_dseg.nii.gz": "derivatives/abcd-hcp-pipeline/sub-{SUBJECTLABEL}/[ses-{SESSIONLABEL}/]anat/sub-{SUBJECTLABEL}[_ses-{SESSIONLABEL}]_space-ACPC_dseg.nii.gz"
}
```

The square brackets around `[_ses-{SESSIONLABEL}/]` and all other session related section implies these blocks are optional, but it should be used if you have multiple sessions for single subjects within a dataset.

These JSON files are organized in a two part system `files_location: files_destination`. The `files_location` is the path under the target directory passed to prepare.py. The `files_destination` is the path under the child directory as dictated by the BIDS format. The child directory will be explained in section X and a link to the BIDS format can be found in the Appendix.

As seen in Example 1, the subject and session lables are filled in with variables. This so the all of the subject (and sessions) found in the lookup CSV can have these files applied to them.

## 4. Using `prepare.py`, `records.py` and `upload.py` Scripts

### `Setting up your system`

When using `prepare.py`,`records.py` and `upload.py` there is a list of things that must exist before the use of the scripts.

1. Python 3.6 needs to be wherever the scripts are being run.  If it is not Python 3.6 you will get an error form the NDA's scripts.  You can either use a system Python 3.6 or a virtual environment with Python 3.6.  ([setting up a virual environment documentation](https://docs.python.org/3.6/tutorial/venv.html)).

2. The Python YAML dictionary must be installed into which ever environment you are runing these scrripts on. ([installing YAML dictionary](https://pypi.org/project/PyYAML/))

3. Install NDA tools into your Python 3.6 environment ([link to the GitHub for nda-tools](https://github.com/NDAR/nda-tools)).  To install `nda-tools`, use the command:

```shell
python3.6 -m pip install nda-tools --user
```

4. Create an upload directory and move all appropriate JSON files and lookup CSV into it.

5. Create a pair of directories under the upload directory. They need to be named `manifest` and `file_mapper`

6. Download the DCAN Labs's `file_mapper_script.py` script.  Clone it into the `file_mapper` directory.  Once it is cloned permissions to the script need to be changed to `755 (-rwxr-xr-x)`.  A link to the DCAN Lab's Gitlab for the file_mapper_script and how to clone it can be found in the Appendix

7. Download the NDA's `nda_manifest.py` script.  Clone it into the `manifest` directory.  Once it is cloned permissions to the script need to be changed to `755 (-rwxr-xr-x)`.  A link to the NDA's Github for the nda_manifest script and how to clone it can be found in the Appendix

### Using `prepare.py`

When using `prepare.py` there are two mandatory flags:

`--destination` (or `-d`): The upload directory mentioned above in set four. This directory is going to be where all of the data will be organized under after data_perpare.py has finished.

`--target` (or `-t`): The directory under which all data the is wanted for upload can be found. This is often a self made directory or the output of a pipeline.

Once this script has been run you will want to check the results. In the upload directory you will find a parent/child directory setup. You should have a parent directory for each of the JSON files. They should have the same name as their corosponding file. Underneither you should find a README, CHANGES, dataset_description.json and child direcotry for ever subject [and session] that was found to have the relevent files listed in the corispoding JSON. If there are no child files under the parent directory then the script couldn't find any of the relavent files listed in the JSON.

The child directory should be labeled thusly.

**`S.X.Y.Z`**

There is also an expected naming convention for the "child" directories.  At the child directory level the naming convention has four "sections", `S.X.Y.Z`.  Three of those are the exact same as above, `"X.Y.Z"`.  The first is `"S."` instead of `"A_"` (the `"."` instead of `"_"` is intentional).

#### Section `S`

Defined like this:

**`sub-<subjectlabel>[_ses-<sessionlabel>]`**

Where:

* `<subjectlabel>` needs to be replaced by your actual BIDS-standard subject label
* `<sessionlabel>` needs to be replaced by your actual BIDS-standard session label
* The square brackets around `[_ses-<sessionlabel>]` implies this block is optional, but it should be used if you have multiple sessions for single subjects within a dataset.

Remember: BIDS labels (`<subjectlabel>` and `<sessionlabel>`) are **ONLY** alphanumeric.  Spaces, underscores, hyphens, and any other seperators are **NOT ALLOWED**.

### Example 1: Prepared Parent and Child Directories

#### `fmriresults01_inputs.anat.T1w`

Below a parent directory named `fmriresults01_inputs.anat.T1w` the scripts will expect any amount of BIDS-formatted standard folders, one for each individual subject or session record you want to upload.  Read on for how the child directories should be formatted.

```ascii
fmriresults01_inputs.anat.T1w/
└── sub-NDARABC123_ses-baseline.inputs.anat.T1w
```

You can see the different sections put together here:

1. `A` is `fmriresults01`
1. `X` is `inputs`
1. `Y` is `anat`
1. `Z` is `T1w`
1. `S` is `sub-NDARABC123_ses-baseline`, (the session is being labeled)
    * `<subjectlabel>` is `NDARABC123`
    * `<sessionlabel>` is `baseline`

#### `sub-NDARABC123_ses-baseline.input.anat.T1w`

Within all child directories there **MUST** be a valid BIDS standard directory hierarchy underneath that follows a standard BIDS folder layout **from the top folder**.

For inputs, use the Official BIDS Validator to check for validity.  For derivatives, remember that there should be `derivatives/<pipeline>` prior to your subject-specific and session-specific derivative folders.

For example, starting with the prepared child directory:

#### `sub-NDARABC123_ses-baseline.input.anat.T1w/sub-NDARABC123/ses-baseline/anat/...`

The final directory structure from the parent directory down should follow like the examples below.

### Example 2: BIDS Anatomical Inputs

```ascii
fmriresults01_inputs.anat.T1w/
└── sub-NDARABC123_ses-baseline.inputs.anat.T1w
    ├── CHANGES
    ├── dataset_description.json
    ├── README
    └── sub-NDARABC123
        └── ses-baseline
            └── anat
                ├── sub-NDARABC123_ses-baseline_T1w.json
                └── sub-NDARABC123_ses-baseline_T1w.nii.gz
```

### Example 3: BIDS Derivatives

```ascii
fmriresults01_derivatives.func.runs_task-rest/
└── sub-NDARABC123_ses-baseline.derivatives.func.runs_task-rest
    └── derivatives
        └── abcd-hcp-pipeline
            └── sub-NDARABC123
                └── ses-baseline
                    └── func
                        ├── sub-NDARABC123_ses-baseline_task-rest_run-1_bold_timeseries.dtseries.nii
                        ├── sub-NDARABC123_ses-baseline_task-rest_run-1_motion.tsv
                        ├── sub-NDARABC123_ses-baseline_task-rest_run-2_bold_timeseries.dtseries.nii
                        └── sub-NDARABC123_ses-baseline_task-rest_run-2_motion.tsv
```

If you directoried do not look to be formatted correctly please check your JSON files for proper formatting.

### Using `records.py`

When using `records.py` there are three mandatory flags:

`--source` (or `-s`): The upload directory mentioned above in set four.

`--lookup` (or `-l`): The lookup flag expects the complete path to the lookup.csv that was covered in part 2 of this README.

`--manifest` (or `-m`): The manifest flag expects the complete path to the `nda_manifest.py` script mentioned at the head of this section of the README.

### Using `upload.py`

When using `upload.py` there are three mandatory flags:

`--collection` (or `-c`): The collection flag needs an **NDA Collection ID**.

`--source` (or `-s`): The source flag expects the complete path to the parent directory from part 1 of this README.  The expected basename of the provided path should have the structure `A_X.Y.Z`.  The script will fail if this is not the case.

`--ndavtcmd` (or `-vt`): The ndavtcmd flag expects the direct path to the `vtcmd` script.

Examples:

```bash
# Maybe it is in your local Python installation binaries folder
~/.local/bin/vtcmd

# Or maybe a Python virtualenv you made in the "..." folder
.../virtualenv/bin/vtcmd
```

## 5. Appendix

### Links for more information

* [BIDS-Formatted Standard Folders](https://github.com/bids-standard/bids-starter-kit/wiki/The-BIDS-folder-hierarchy)
* [Cloning a GitHub repository](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository)
* [Cloning a GitLab repository](https://docs.gitlab.com/ee/gitlab-basics/start-using-git.html)
* [file_mapper_script.py GitLab](https://gitlab.com/Fair_lab/file_mapper)
* [NDA fmriresults01 data structure](https://nda.nih.gov/data_structure.html?short_name=fmriresults01)
* [NDA imagingcollection01 data structure](https://nda.nih.gov/data_structure.html?short_name=imagingcollection01)
* [nda_manifest.py GitHub](https://github.com/NDAR/manifest-data)
* [NDA Tools GitHub](https://github.com/NDAR/nda-tools)
* [Python 3 Virtual Environment](https://docs.python.org/3.6/tutorial/venv.html)
* [Python YAML dictionary installation](https://pypi.org/project/PyYAML/)
* [Section `X` Guide](https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html)
* [Section `Y` Existing Entities](https://bids-specification.readthedocs.io/en/stable/99-appendices/04-entity-table.html)

### Glossary

* BIDS: Brain Imaging Data Structure
* fMRI: Functional Magnetic Resonance Imaging
* NDA: NIMH Data Archive
* NIMH: National Institute of Mental Health
* `virtualenv`: A stand in for a Python virtual environment directory
